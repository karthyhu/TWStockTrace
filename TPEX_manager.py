import json
import requests
import os
import time
import random
import re
import itertools
import timenormalyize as tn



data_list = {
    '02':'é£Ÿå“å·¥æ¥­',
    '03':'å¡‘è† å·¥æ¥­',
    '04':'ç´¡ç¹”çº–ç¶­',
    '05':'é›»æ©Ÿæ©Ÿæ¢°',
    '06':'é›»å™¨é›»çºœ',
    '21':'åŒ–å­¸å·¥æ¥­',
    '08':'ç»ç’ƒé™¶ç“·',
    '10':'é‹¼éµå·¥æ¥­',
    '11':'æ©¡è† å·¥æ¥­',
    '14':'å»ºæç‡Ÿé€ ',
    '15':'èˆªé‹æ¥­',
    '16':'è§€å…‰é¤æ—…',
    '17':'é‡‘èæ¥­',
    '20':'å…¶ä»–',
    '22':'ç”ŸæŠ€é†«ç™‚é¡',
    '23':'æ²¹é›»ç‡ƒæ°£é¡',
    '24':'åŠå°é«”é¡',
    '25':'é›»è…¦åŠé€±é‚Šé¡',
    '26':'å…‰é›»æ¥­é¡',
    '27':'é€šä¿¡ç¶²è·¯é¡',
    '28':'é›»å­é›¶çµ„ä»¶é¡',
    '29':'é›»å­é€šè·¯é¡',
    '30':'è³‡è¨Šæœå‹™é¡',
    '31':'å…¶ä»–é›»å­é¡',
    '32':'æ–‡åŒ–å‰µæ„æ¥­',
    '33':'è¾²æ¥­ç§‘æŠ€æ¥­',
    '35':'ç¶ èƒ½ç’°ä¿é¡',
    '36':'æ•¸ä½é›²ç«¯é¡',
    '37':'é‹å‹•ä¼‘é–’é¡',
    '38':'å±…å®¶ç”Ÿæ´»é¡',
    '80':'ç®¡ç†è‚¡ç¥¨',
    'AA':'å—ç›Šè­‰åˆ¸',
    'EE':'ä¸Šæ«ƒETF',
    'EN':'æŒ‡æ•¸æŠ•è³‡è­‰åˆ¸(ETN)',
    'TD':'å°ç£å­˜è¨—æ†‘è­‰(TDR)',
    'WW':'èªè³¼å”®æ¬Šè­‰',
    'GG':'èªè‚¡æ¬Šæ†‘è­‰',
    'BC':'ç‰›ç†Šè­‰(ä¸å«å±•å»¶å‹ç‰›ç†Šè­‰)',
    'XY':'å±•å»¶å‹ç‰›ç†Šè­‰',
    'EW':'æ‰€æœ‰è­‰åˆ¸(ä¸å«æ¬Šè­‰ã€ç‰›ç†Šè­‰)',
    'AL':'æ‰€æœ‰è­‰åˆ¸',
    'OR':'å§”è¨—åŠæˆäº¤è³‡è¨Š(16:05æä¾›)'
}

headers = {
  'Accept': 'application/json, text/javascript, */*; q=0.01',
  'Accept-Language': 'zh-TW,zh;q=0.9,en-US;q=0.8,en;q=0.7',
  'Connection': 'keep-alive',
  'Content-Type': 'application/x-www-form-urlencoded; charset=UTF-8',
  'Origin': 'https://www.tpex.org.tw',
  'Referer': 'https://www.tpex.org.tw/zh-tw/mainboard/trading/info/mi-pricing.html',
  'Sec-Fetch-Dest': 'empty',
  'Sec-Fetch-Mode': 'cors',
  'Sec-Fetch-Site': 'same-origin',
  'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/138.0.0.0 Safari/537.36',
  'X-Requested-With': 'XMLHttpRequest',
  'sec-ch-ua': '"Not)A;Brand";v="8", "Chromium";v="138", "Google Chrome";v="138"',
  'sec-ch-ua-mobile': '?0',
  'sec-ch-ua-platform': '"Windows"'
}


class TPEX_manager:
    def __init__(self, daily_data_dir='./raw_stock_data/daily/tpex'):
        if not os.path.exists(daily_data_dir):
            os.makedirs(daily_data_dir)
        self.daily_data_dir = daily_data_dir
        
        # 'ä»£è™Ÿ', 'åç¨±', 'æ”¶ç›¤', 'æ¼²è·Œ', 'é–‹ç›¤', 'æœ€é«˜', 'æœ€ä½', 'æˆäº¤è‚¡æ•¸', 'æˆäº¤é‡‘é¡(å…ƒ)', 'æ¼²å¹…%'
        self.fill_list = ['Code', 'Name', 'ClosingPrice', 'Change', 'OpeningPrice', 'HighestPrice', 'LowestPrice', 'TradeVolume', 'TradeValue', 'Range']

        self.session = requests.Session()
        self.session.headers.update(headers)
        try:
            self.session.get('https://www.tpex.org.tw/zh-tw/mainboard/trading/info/pricing.html', timeout=10)
            print("âœ… æœƒè©±å·²å»ºç«‹ï¼ŒCookie å·²è‡ªå‹•å„²å­˜ã€‚")
            time.sleep(random.uniform(0.2, 0.5)) # æ¨¡æ“¬çŸ­æš«åœç•™
        except requests.exceptions.RequestException as e:
            print(f"âŒ æ­¥é©Ÿä¸€å¤±æ•—ï¼Œç„¡æ³•å»ºç«‹æœƒè©±: {e}")

    def save_file(self, data, filename:str = 'Noname'):
        with open(f'{self.daily_data_dir}/{filename}.json', 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=1)

    def safe_float(self, value):
        try:
          return float(value)
        except ValueError:
          return 0.0

    def safe_int(self, value):
        try:
          return int(value)
        except ValueError:
          return 0

    def gan_url(self, date: str, type: str = 'AL') -> str:
        # print(f"æ­£åœ¨ç”Ÿæˆ URLï¼Œæ—¥æœŸ: {date}, é¡å‹: {type}")
        sdate = date.split('/')
        url = f'https://www.tpex.org.tw/www/zh-tw/afterTrading/otc?date={sdate[0]}%2F{sdate[1]}%2F{sdate[2]}&type={type}&id=&response=json&order=0&sort=asc'
        # print(f"URL: {url}")
        return url


    def _process_stock_item(self, item):
        """è™•ç†å–®å€‹è‚¡ç¥¨é …ç›®æ•¸æ“šçš„å…±ç”¨æ–¹æ³•"""
        # åªå–å‰9å€‹æ¬„ä½
        item = item[:9]
        # æ¸…ç†æ•¸æ“šï¼Œç§»é™¤åŠ è™Ÿå’Œé€—è™Ÿ
        item = [re.sub(r'[+,]', '', str(field)) for field in item]
        
        # è¨ˆç®—æ¼²å¹…
        closeprice = self.safe_float(item[self.fill_list.index('ClosingPrice')])
        change = self.safe_float(item[self.fill_list.index('Change')])
        range_percent = (change / closeprice * 100) if closeprice != 0 else 0.0
        
        # æ·»åŠ æ¼²å¹…åˆ°é …ç›®æœ«å°¾
        item.append(str(round(range_percent, 3)))

        return item

    def _fetch_and_parse_data(self, date, type_code='AL'):
        """ç²å–ä¸¦è§£æè‚¡ç¥¨æ•¸æ“šçš„å…±ç”¨æ–¹æ³•"""
        try:
            res = self.session.get(self.gan_url(date, type_code), timeout=10)
            data = json.loads(res.text)
            return data['tables'][0]
        except Exception as e:
            print(f'Error fetching data for {type_code}: {e}')
            return None

    def download_get_once(self, date: str = '114/07/14'):
        date = tn.normalize_date(date, "ROC", "/")
        """ä¸‹è¼‰å–®ä¸€é¡å‹(AL)çš„è‚¡ç¥¨æ•¸æ“š"""
        table_data = self._fetch_and_parse_data(date)
        
        if not table_data:
            print(f'Failed to fetch data for date {date}')
            return None
            
        try:
            totaldata = {
                'date': table_data['date'].replace('/', ''),
                'fields': self.fill_list.copy(),
                'data': {}
            }
            
            for item in table_data['data']:
                processed_item = self._process_stock_item(item)
                totaldata['data'][processed_item[0]] = processed_item
                
            self.save_file(totaldata, totaldata['date'])
            print(f"âœ… Successfully downloaded {len(totaldata['data'])} records for {totaldata['date']}")
            return totaldata
            
        except Exception as e:
            print(f'Error processing data: {e}')
            return False

    def download_get_loop(self, date: str, max_types: int = None):
        if max_types is None:
            max_types = list(data_list.keys()).index('TD') + 1
    
        """ä¸‹è¼‰å¤šç¨®é¡å‹è‚¡ç¥¨æ•¸æ“šçš„å¾ªç’°æ–¹æ³•"""
        totaldata = {
            'date': '',
            'fields': self.fill_list.copy(),
            'data': {}
        }
        total_count = 0
        
        # print(f"Starting download for {max_types} stock types...")
        
        for key, value in itertools.islice(data_list.items(), max_types):
            print(f'.', end=' ')
            # print(f"æ­£åœ¨è™•ç†é¡å‹: {key} - {value}")
            
            table_data = self._fetch_and_parse_data(date, key)
            
            if table_data:
                try:
                    # è¨­å®šæ—¥æœŸï¼ˆåªéœ€è¦è¨­å®šä¸€æ¬¡ï¼‰
                    if not totaldata['date']:
                        totaldata['date'] = table_data['date'].replace('/', '')
                    
                    # è™•ç†è©²é¡å‹çš„æ‰€æœ‰è‚¡ç¥¨æ•¸æ“š
                    for item in table_data['data']:
                        processed_item = self._process_stock_item(item)
                        totaldata['data'][processed_item[0]] = processed_item
                    
                    total_count += len(table_data['data'])                    
                except Exception as e:
                    print(f'âŒ Error processing {key}: {e}, at {key}, {value}')
                    return False
            else:
                print('âŒ Failed to fetch')
                
            # çŸ­æš«å»¶é²é¿å…éæ–¼é »ç¹çš„è«‹æ±‚
            time.sleep(random.uniform(0.1, 0.3))
        print(f'done, datacount = {total_count}')
        # print(f"\nğŸ“Š Total records downloaded: {total_count}")
        # print(f"ğŸ“Š Unique stocks processed: {len(totaldata['data'])}")
        
        if totaldata['data']:
            self.save_file(totaldata, totaldata['date'])
            return totaldata
        else:
            print("No data was successfully downloaded")
            return None

    def download_toc_post(self, date:str):
        session = requests.Session()
        session.headers.update(headers)
        entry_url = "https://www.tpex.org.tw/zh-tw/mainboard/trading/info/mi-pricing.html"
        url = 'https://www.tpex.org.tw/www/zh-tw/afterTrading/otc'
        try:
            print("æ­¥é©Ÿä¸€ï¼šæ­£åœ¨è¨ªå•å…¥å£é é¢ä»¥å»ºç«‹æœƒè©±...")
            session.get(entry_url, timeout=10)
            print("âœ… æœƒè©±å·²å»ºç«‹ï¼ŒCookie å·²è‡ªå‹•å„²å­˜ã€‚")
            time.sleep(random.uniform(1, 3)) # æ¨¡æ“¬çŸ­æš«åœç•™
        except requests.exceptions.RequestException as e:
            print(f"âŒ æ­¥é©Ÿä¸€å¤±æ•—ï¼Œç„¡æ³•å»ºç«‹æœƒè©±: {e}")

        payload = {
            'date': date,  # æ”¹ä½ è¦çš„æ—¥æœŸ
            'type': '',
            'id': '',
            'response': 'json'
        }

        totaldata = {}
        totaldata['date'] = payload['date'].replace('/', '')
        totaldata['fields'] = ['ä»£è™Ÿ', 'åç¨±', 'æ”¶ç›¤ ', 'æ¼²è·Œ', 'é–‹ç›¤ ', 'æœ€é«˜ ', 'æœ€ä½', 'æˆäº¤è‚¡æ•¸  ', ' æˆäº¤é‡‘é¡(å…ƒ)', ' æˆäº¤ç­†æ•¸ ', 'æœ€å¾Œè²·åƒ¹', 'æœ€å¾Œè²·é‡<br>(å¼µæ•¸)', 'æœ€å¾Œè³£åƒ¹', 'æœ€å¾Œè³£é‡<br>(å¼µæ•¸)', 'ç™¼è¡Œè‚¡æ•¸ ', 'æ¬¡æ—¥æ¼²åœåƒ¹ ', 'æ¬¡æ—¥è·Œåœåƒ¹', 'æ¼²å¹…']
        totaldata['data'] = []
        totalCount = 0
        for key, value in data_list.items():
            payload['type'] = key
            try:
                response = requests.post(url, headers=headers, data=payload)
                data = response.json()

                # closeprice = float(data['tables'][0][2])
                # change = float(data['tables'][0][3])
                # range = (change / closeprice * 100) if closeprice != 0 else 0
                # data['tables'][0]['data'].append(range)
                # totaldata['data'].extend(data['tables'][0]['data'])
                # totalCount += int(data['tables'][0]['totalCount'])
                print('.', end=' ')
            except Exception as e:
                print(f"key = {key}, {value}")
                print(f'Error: {e}')
            # time.sleep(random.uniform(0.2, 1.0))
            # time.sleep(0.1)
        print(f"total cnt = {totalCount}")

        with open(f"{payload['date'].replace('/', '')}.json", 'w', encoding='utf-8') as f:
            json.dump(totaldata, f, ensure_ascii=False, indent=4)
            print("save done")

    def genpassdayfile(self, start_date: str, during_days: int = 2):
        all_files = os.listdir(self.daily_data_dir)
        all_files = [f for f in all_files if f.endswith('.json')]
        #é€†åº
        all_files.sort(reverse=True)
        start_date = tn.normalize_date(start_date, "ROC", "")
        start_index = all_files.index(f'{start_date}.json')
        for i in range(during_days):
            with open(f'{self.daily_data_dir}/{all_files[start_index + i]}', 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            with open(f'{self.daily_data_dir}/T{i+1}_Day.json', 'w', encoding='utf-8') as f:
                json.dump(data, f, ensure_ascii=False, indent=1)


def daily_trace(date: str = None):
    # å–å¾—ä»Šå¤©æ—¥æœŸ
    if date is None:
        date = tn.get_current_date("ROC", "")
    
    date = tn.normalize_date(date, "ROC", "")
        
    # å„²å­˜ä»Šå¤©çš„è³‡æ–™
    trace_manager = TPEX_manager()
    data = trace_manager.download_get_once(date)
    if data:
        trace_manager.save_file(data, 'today')
        print(f"{date} çš„è³‡æ–™å·²æˆåŠŸå„²å­˜ã€‚")
    trace_manager.genpassdayfile(date, 2)


if __name__ == "__main__":
    # æ¸¬è©¦ç”¨
    # daily_trace('114/07/17')
    t = TPEX_manager()
    # t.download_get_loop('114/07/14')
    # t.download_get_once('114/07/15')
    # t.genpassdayfile('114/08/05', 2)

